{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = [ \"i like dog\", \"i like cat\", \"i like animal\",\n              \"dog cat animal\", \"apple cat dog like\", \"dog fish milk like\",\n              \"dog cat eyes like\", \"i like apple\", \"apple i hate\",\n              \"apple i movie book music like\", \"cat dog hate\", \"cat dog like\"]\n\ndtype = torch.FloatTensor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_sequence = \" \".join(sentences).split()\nword_list = \" \".join(sentences).split()\nword_list = list(set(word_list))\nword_dict = {w: i for i, w in enumerate(word_list)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_sequence, len(word_sequence), word_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word2Vec Parameter\nbatch_size = 20  # To show 2 dim embedding graph\nembedding_size = 2  # To show 2 dim embedding graph\nvoc_size = len(word_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make skip gram of one size window\nskip_grams = []\nfor i in range(1, len(word_sequence) - 1):\n    target = word_dict[word_sequence[i]]\n    context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n\n    for w in context:\n        skip_grams.append([target, w])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skip_grams, len(skip_grams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_batch(data, size):\n    random_inputs = []\n    random_labels = []\n    random_index = np.random.choice(range(len(data)), size, replace=False)\n\n    for i in random_index:\n        random_inputs.append(np.eye(voc_size)[data[i][0]])  # target\n        random_labels.append(data[i][1])  # context word\n\n    return random_inputs, random_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_batch, target_batch = random_batch(skip_grams, 20)\ninput_batch, target_batch, len(input_batch), len(input_batch[0]), len(target_batch) # , skip_grams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Word2Vec(nn.Module):\n    def __init__(self):\n        super(Word2Vec, self).__init__()\n\n        # W and WT is not Traspose relationship\n        self.W = nn.Parameter(-2 * torch.rand(voc_size, embedding_size) + 1).type(dtype) # voc_size > embedding_size Weight\n        self.WT = nn.Parameter(-2 * torch.rand(embedding_size, voc_size) + 1).type(dtype) # embedding_size > voc_size Weight\n\n    def forward(self, X):\n        # X : [batch_size, voc_size]\n        hidden_layer = torch.matmul(X, self.W) # hidden_layer : [batch_size, embedding_size]\n        output_layer = torch.matmul(hidden_layer, self.WT) # output_layer : [batch_size, voc_size]\n        return output_layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Word2Vec()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training\nfor epoch in range(5000):\n\n    input_batch, target_batch = random_batch(skip_grams, batch_size)\n\n    input_batch = torch.Tensor(input_batch)\n    target_batch = torch.LongTensor(target_batch)\n\n    optimizer.zero_grad()\n    output = model(input_batch)\n\n    # output : [batch_size, voc_size], target_batch : [batch_size] (LongTensor, not one-hot)\n    loss = criterion(output, target_batch)\n    if (epoch + 1)%1000 == 0:\n        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n\n    loss.backward()\n    optimizer.step()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, label in enumerate(word_list):\n    W, WT = model.parameters()\n    x,y = float(W[i][0]), float(W[i][1])\n    plt.scatter(x, y)\n    plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}