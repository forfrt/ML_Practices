{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtype = torch.FloatTensor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Text-CNN Parameter\nembedding_size = 2 # n-gram\nsequence_length = 3\nnum_classes = 2  # 0 or 1\nfilter_sizes = [2, 2, 2] # n-gram window\nnum_filters = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3 words sentences (=sequence_length is 3)\nsentences = [\"i love you\", \"he loves me\", \"she likes baseball\", \"i hate you\", \"sorry for that\", \"this is awful\"]\nlabels = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is not good.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_list = \" \".join(sentences).split()\nword_list = list(set(word_list))\nword_dict = {w: i for i, w in enumerate(word_list)}\nvocab_size = len(word_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = []\nfor sen in sentences:\n    inputs.append(np.asarray([word_dict[n] for n in sen.split()]))\n\ntargets = []\nfor out in labels:\n    targets.append(out) # To using Torch Softmax Loss function\n\ninput_batch = torch.LongTensor(inputs)\ntarget_batch = torch.LongTensor(targets)\n\ninput_batch, target_batch, input_batch.size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextCNN(nn.Module):\n    def __init__(self):\n        super(TextCNN, self).__init__()\n\n        self.num_filters_total = num_filters * len(filter_sizes)\n        self.W = nn.Parameter(torch.empty(vocab_size, embedding_size).uniform_(-1, 1)).type(dtype)\n        self.Weight = nn.Parameter(torch.empty(self.num_filters_total, num_classes).uniform_(-1, 1)).type(dtype)\n        self.Bias = nn.Parameter(0.1 * torch.ones([num_classes])).type(dtype)\n\n    def forward(self, X):\n        embedded_chars = self.W[X] # [batch_size, sequence_length, sequence_length]\n        embedded_chars = embedded_chars.unsqueeze(1) # add channel(=1) [batch, channel(=1), sequence_length, embedding_size]\n\n        pooled_outputs = []\n        for filter_size in filter_sizes:\n            # conv : [input_channel(=1), output_channel(=3), (filter_height, filter_width), bias_option]\n            conv = nn.Conv2d(1, num_filters, (filter_size, embedding_size), bias=True)(embedded_chars)\n            h = F.relu(conv)\n            # mp : ((filter_height, filter_width))\n            mp = nn.MaxPool2d((sequence_length - filter_size + 1, 1))\n            # pooled : [batch_size(=6), output_height(=1), output_width(=1), output_channel(=3)]\n            pooled = mp(h).permute(0, 3, 2, 1)\n            pooled_outputs.append(pooled)\n\n        h_pool = torch.cat(pooled_outputs, len(filter_sizes)) # [batch_size(=6), output_height(=1), output_width(=1), output_channel(=3) * 3]\n        h_pool_flat = torch.reshape(h_pool, [-1, self.num_filters_total]) # [batch_size(=6), output_height * output_width * (output_channel * 3)]\n\n        model = torch.mm(h_pool_flat, self.Weight) + self.Bias # [batch_size, num_classes]\n        return model\n\nmodel = TextCNN()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_W=nn.Parameter(torch.empty(vocab_size, embedding_size).uniform_(-1, 1)).type(dtype)\ntest_embedded=test_W[input_batch]\n\ntest_embedded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training\nfor epoch in range(5000):\n    optimizer.zero_grad()\n    output = model(input_batch)\n\n    # output : [batch_size, num_classes], target_batch : [batch_size] (LongTensor, not one-hot)\n    loss = criterion(output, target_batch)\n    if (epoch + 1) % 1000 == 0:\n        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n\n    loss.backward()\n    optimizer.step()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test\ntest_text = 'sorry hate you'\ntests = [np.asarray([word_dict[n] for n in test_text.split()])]\ntest_batch = Variable(torch.LongTensor(tests))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict\npredict = model(test_batch).data.max(1, keepdim=True)[1]\nif predict[0][0] == 0:\n    print(test_text,\"is Bad Mean...\")\nelse:\n    print(test_text,\"is Good Mean!!\")\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}